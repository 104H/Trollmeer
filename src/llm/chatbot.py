from typing import List

from langchain.chat_models import init_chat_model
from langchain_community.document_loaders import JSONLoader
from langchain_core.documents import Document
from langchain_core.messages import AIMessage
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_openai import OpenAIEmbeddings
from langchain_tavily import TavilySearch
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.graph import START, StateGraph
from langgraph.prebuilt import ToolNode
from prompttemplate import custom_rag_prompt
from typing_extensions import TypedDict


# the state of our llm
# the state here is in the context of a state machine
# as in theory of computation
class State(TypedDict):
    question: str
    context: List[Document]
    answer: str


loader = JSONLoader(
    file_path="data/preprocessed_data", jq_schema=".messages", text_content=False
)

# the prompt is pulled from the openai repository
prompt = custom_rag_prompt

# text embeddings
embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
# store the emdeddings in a vector store
vector_store = InMemoryVectorStore(embeddings)


class ChatBot:
    # the graph builder
    graph_builder = StateGraph(State)

    # the model we will use
    llm = init_chat_model("openai:gpt-4.1")

    # tools for the model
    tools = [TavilySearch(max_results=2, topic="general")]

    # the memory of the llm to maintain state
    memory = InMemorySaver()

    # load the chat data
    data = loader.load()

    def __init__(self):
        # bind search tool with llm
        self.llm = self.llm.bind_tools(self.tools)

        # create a tool node and add the tools as a graph node as a list
        tool_node = ToolNode(self.tools)
        self.graph_builder.add_node("tools", tool_node)

        # chatbot recieves a new state at every call
        # the state holds a message in it
        def query(state: State):
            return {"messages": [self.llm.invoke(state["messages"])]}

        # add a retrieve and generate sequence for rag
        self.graph_builder.add_sequence([self.retrieve, self.generate])

        # Start point connects to the chatbot
        self.graph_builder.add_edge(START, "retrieve")

        # add a conditional end to the search tool
        # self.graph_builder.add_conditional_edges("retrieve", tools_condition)

        # send the tool output back to the llm to process it into natural lang
        # self.graph_builder.add_edge("tools", "retrieve")

        # node chatbot connects to the end othe graph
        # self.graph_builder.add_edge("generate", END)

        # add the memory saver and
        # compile the graph
        self.graph = self.graph_builder.compile(checkpointer=self.memory)

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=30,  # chunk size (characters)
            chunk_overlap=6,  # chunk overlap (characters)
            add_start_index=True,  # track index in original document
        )
        # apply the above splitter on the data
        all_splits = text_splitter.split_documents(self.data)
        self.document_ids = vector_store.add_documents(documents=all_splits)

    # interactive graph streamer
    def stream_graph_updates(self, user_input: str):
        # use one thread for maintaining state
        config = {"configurable": {"thread_id": "1"}}

        # a graph stream is given a dictionary with user role
        # and the message the user sent
        graph_input = {"messages": [{"role": "user", "content": user_input}]}

        # send input to the graph
        for event in self.graph.stream(graph_input, config):
            for value in event.values():
                last_msg = value["messages"][-1]

                # print the message only if it is generated by the llm
                # without this filer we see tool messages
                if isinstance(last_msg, AIMessage):
                    print("Assistant:", last_msg.content)

    def interactor(self):
        print("Type quit or q to close the conversation.")

        # infinite loop for recieving input
        while True:
            # recieve typed input
            user_input = input("Your message: ")

            # enter on the these options to end the conversation
            if user_input.lower() in ["quit", "q"]:
                print("Until next time!")
                break

            # if the conversation was not ended, input the text to the model
            self.stream_graph_updates(user_input)

    def retrieve(self, state: State):
        """Retrieve docs from the vectors store"""
        retrieved_docs = vector_store.similarity_search(state["question"])
        return {"context": retrieved_docs}

    def generate(self, state: State):
        """Generate a response using the docs"""
        docs_content = "\n\n".join(doc.page_content for doc in state["context"])
        messages = prompt.invoke(
            {"question": state["question"], "context": docs_content}
        )
        response = self.llm.invoke(messages)
        return {"answer": response.content}
